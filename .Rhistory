df = pd.json_normalize(all['sentences'], 'tokens')
View(df)
type(all)
all = df.loc[:5]['combined_text'].tolist()
all = df.loc[:5]['combined_text']
df = pd.read_csv("data_for_embeddings.csv")
all = df.loc[:5]['combined_text']
all = df.loc[:5]['combined_text'].todict()
all = df.loc[:5]['combined_text'].to_dict()
all = p.posdep(all)
for element in all
print(element)
for element in all
print(element)
all = df.loc[:5]['combined_text'].to_dict()
View(all)
all = p.posdep(all)
all = 'Hello my name is Olivier and I\'im trying something here.'
pos = p.posdep(all)
pos = p.posdep(all)
df = pd.json_normalize(pos['sentences'], 'tokens')
all = df.loc[:5]['combined_text'].to_dict()
df = pd.read_csv("data_for_embeddings.csv")
all = df.loc[:5]['combined_text'].to_dict()
pos = p.posdep(all)
type(all)
pos = p.posdep(all)
from trankit import Pipeline
import pandas as pd
from pandas import json_normalize
# Read the CSV file into a DataFrame
df = pd.read_csv("data_for_embeddings.csv")
# Initialize a pipeline for English
p = Pipeline('english')
# Create an empty list to store the results
results = []
# Iterate through each row of the DataFrame and process the text
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
results.append(pos)
# Normalize the results into a DataFrame
df_results = pd.json_normalize(results, 'sentences', 'tokens')
# Now, df_results contains the POS and dependency parsing results
print(df_results)
df_results = pd.json_normalize(results, 'sentences', 'tokens')
df_results = pd.json_normalize(results, 'sentences', 'tokens', errors='ignore')
View(pos)
df.loc[:5]['combined_text'][1]
type(df.loc[:5]['combined_text'][1]  )
p.podep(df.loc[:5]['combined_text'][1])
p.posdep(df.loc[:5]['combined_text'][1])
results = []
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
results.append(pos)
View(results)
type(p.posdep(df.loc[:5]['combined_text'][1]))
test = p.posdep(df.loc[:5]['combined_text'][1])
testdf = pd.DataFrame(test)
View(testdf)
View(test)
testdf = pd.DataFrame(test['sentences'])
View(testdf)
testdf = pd.DataFrame(test['sentences'], 'tokens')
test = p.posdep(df.loc[:5]['combined_text'][1])
testdf = pd.DataFrame(test['sentences'], 'tokens')
testdf = pd.DataFrame(pd.json_normalize(test['sentences'], 'tokens'))
View(testdf)
results = []
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results.append(pos)
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results.append(pos_df)
results = []
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results,pos_df])
View(results)
View(pos_df)
results = pd.DataFrame()  # Initialisez un DataFrame vide
results = pd.DataFrame()  # Initialisez un DataFrame vide
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
# Utilisez pd.concat pour ajouter pos_df à results
results = pd.concat([results, pos_df], ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
View(results)
results = pd.DataFrame()  # Initialisez un DataFrame vide en dehors de la boucle
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
# Utilisez pd.concat pour ajouter pos_df au DataFrame results
results = pd.concat([results, pos_df], ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
results_list = []  # Créez une liste pour stocker temporairement les DataFrames individuels
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)  # Ajoutez chaque DataFrame individuel à la liste
# Utilisez pd.concat pour concaténer tous les DataFrames de la liste en un seul DataFrame
results = pd.concat(results_list, ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
results_list = []  # Créez une liste pour stocker temporairement les DataFrames individuels
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)  # Ajoutez chaque DataFrame individuel à la liste
# Utilisez pd.concat pour concaténer tous les DataFrames de la liste en un seul DataFrame
results = pd.concat(results_list, ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
results_list = []  # Créez une liste pour stocker temporairement les DataFrames individuels
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)  # Ajoutez chaque DataFrame individuel à la liste
# Utilisez pd.concat pour concaténer tous les DataFrames de la liste en un seul DataFrame
results = pd.concat(results_list, axis=0, ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
results_list = []  # Créez une liste pour stocker temporairement les DataFrames individuels
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)  # Ajoutez chaque DataFrame individuel à la liste
results = pd.concat(results_list, axis=0, ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
results_list = []  # Créez une liste pour stocker temporairement les DataFrames individuels
results = []
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)  # Ajoutez chaque DataFrame individuel à la liste
results = pd.concat(results,results_list)
# Maintenant, results contient tous les résultats concaténés
print(results)
results_list = []  # Créez une liste pour stocker temporairement les DataFrames individuels
# Supposons que vous ayez déjà importé la bibliothèque p et défini df
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)  # Ajoutez chaque DataFrame individuel à la liste
# Concaténez tous les résultats de la liste results_list en un seul DataFrame
results = pd.concat(results_list)
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
# Supposons que vous ayez déjà importé la bibliothèque p et défini df
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df], ignore_index=True)
# Maintenant, results contient tous les résultats concaténés
print(results)
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
# Supposons que vous ayez déjà importé la bibliothèque p et défini df
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results.concat(pos_df)
# Maintenant, results contient tous les résultats concaténés
print(results)
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
type(pos_df)
results.concat(pos_df)
# Maintenant, results contient tous les résultats concaténés
print(results)
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = results.concat(pos_df)
# Maintenant, results contient tous les résultats concaténés
print(results)
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
# Supposons que vous ayez déjà importé la bibliothèque p et défini df
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results,pos_df])
# Maintenant, results contient tous les résultats concaténés
print(results)
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
# Supposons que vous ayez déjà importé la bibliothèque p et défini df
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df[text] = pd.json_normalize(pos['sentences'], 'tokens')
#results = pd.concat([results,pos_df])
# Maintenant, results contient tous les résultats concaténés
print(results)
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
# Supposons que vous ayez déjà importé la bibliothèque p et défini df
for text in df.loc[:3]['combined_text']:
i=1
pos = p.posdep(text)
pos = p.posdep(text)
pos_df[i] = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
results = pd.DataFrame()  # Créez un DataFrame vide pour stocker les résultats finaux
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
results = pd.DataFrame()
for i in range(len(df)):
pos = df.loc[i]['combined_text']
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.DataFrame()
for i in range(len(df)):
i=0
pos = df.loc[i]['combined_text']
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
i=i+1
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results,pos_df])
results_list = []
for text in df.loc['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
# Après la boucle, concaténez tous les résultats en un seul DataFrame
results = pd.concat(results_list)
results_list = []
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
# Après la boucle, concaténez tous les résultats en un seul DataFrame
results = pd.concat(results_list)
import pandas as pd
import time
import pandas as pd
import time
# Supposons que vous ayez déjà défini df et p ici
# Mesurer le temps d'exécution du premier bout de code
results = pd.DataFrame()
start_time = time.time()
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
end_time = time.time()
execution_time_1 = end_time - start_time
# Réinitialiser le DataFrame des résultats
results = None
# Mesurer le temps d'exécution du deuxième bout de code
results_list = []
start_time = time.time()
for text in df.loc[:4]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
results = pd.concat(results_list)
end_time = time.time()
execution_time_2 = end_time - start_time
# Afficher les temps d'exécution
print("Temps d'exécution du premier bout de code :", execution_time_1)
print("Temps d'exécution du deuxième bout de code :", execution_time_2)
import pandas as pd
import time
# Supposons que vous ayez déjà défini df et p ici
# Mesurer le temps d'exécution du premier bout de code
results = pd.DataFrame()
start_time = time.time()
for text in df.loc[:4]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
end_time = time.time()
execution_time_1 = end_time - start_time
# Réinitialiser le DataFrame des résultats
results = None
# Mesurer le temps d'exécution du deuxième bout de code
results_list = []
start_time = time.time()
for text in df.loc[:4]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
results = pd.concat(results_list)
end_time = time.time()
execution_time_2 = end_time - start_time
# Afficher les temps d'exécution
print("Temps d'exécution du premier bout de code :", execution_time_1)
print("Temps d'exécution du deuxième bout de code :", execution_time_2)
import pandas as pd
import time
# Supposons que vous ayez déjà défini df et p ici
# Mesurer le temps d'exécution du premier bout de code
results = pd.DataFrame()
start_time = time.time()
for text in df.loc[:10]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
end_time = time.time()
execution_time_1 = end_time - start_time
# Réinitialiser le DataFrame des résultats
results = None
# Mesurer le temps d'exécution du deuxième bout de code
results_list = []
start_time = time.time()
for text in df.loc[:10]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
results = pd.concat(results_list)
end_time = time.time()
execution_time_2 = end_time - start_time
# Afficher les temps d'exécution
print("Temps d'exécution du premier bout de code :", execution_time_1)
print("Temps d'exécution du deuxième bout de code :", execution_time_2)
import pandas as pd
import time
from trankit import Pipeline
from pandas import json_normalize
df = pd.read_csv("data_for_embeddings.csv")
# initialize a pipeline for English
p = Pipeline('english')
# Supposons que vous ayez déjà défini df et p ici
# Mesurer le temps d'exécution du premier bout de code
results = pd.DataFrame()
start_time = time.time()
for text in df.loc[:20]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
end_time = time.time()
execution_time_1 = end_time - start_time
# Réinitialiser le DataFrame des résultats
results = None
# Mesurer le temps d'exécution du deuxième bout de code
results_list = []
start_time = time.time()
for text in df.loc[:20]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
results = pd.concat(results_list)
end_time = time.time()
execution_time_2 = end_time - start_time
# Afficher les temps d'exécution
print("Temps d'exécution du premier bout de code :", execution_time_1)
print("Temps d'exécution du deuxième bout de code :", execution_time_2)
import pandas as pd
import time
from trankit import Pipeline
from pandas import json_normalize
df = pd.read_csv("data_for_embeddings.csv")
# initialize a pipeline for English
p = Pipeline('english')
# Supposons que vous ayez déjà défini df et p ici
# Mesurer le temps d'exécution du premier bout de code
results = pd.DataFrame()
start_time = time.time()
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results, pos_df])
end_time = time.time()
execution_time_1 = end_time - start_time
# Réinitialiser le DataFrame des résultats
results = None
# Mesurer le temps d'exécution du deuxième bout de code
results_list = []
start_time = time.time()
for text in df.loc[:5]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results_list.append(pos_df)
results = pd.concat(results_list)
end_time = time.time()
execution_time_2 = end_time - start_time
# Afficher les temps d'exécution
print("Temps d'exécution du premier bout de code :", execution_time_1)
print("Temps d'exécution du deuxième bout de code :", execution_time_2)
results = pd.DataFrame()
for text in df.loc[:3]['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results,pos_df])
View(results)
from trankit import Pipeline
import pandas as pd
from pandas import json_normalize
df = pd.read_csv("data_for_embeddings.csv")
# initialize a pipeline for English
p = Pipeline('english')
#test = p.posdep(df.loc[:5]['combined_text'][1])
#testdf = pd.DataFrame(pd.json_normalize(test['sentences'], 'tokens'))
#pos = p.posdep(all)
results = pd.DataFrame()
for text in df.loc['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results,pos_df])
results.to_csv("annotated_trankit.csv")
from trankit import Pipeline
import pandas as pd
from pandas import json_normalize
df = pd.read_csv("data_for_embeddings.csv")
# initialize a pipeline for English
p = Pipeline('english')
#test = p.posdep(df.loc[:5]['combined_text'][1])
#testdf = pd.DataFrame(pd.json_normalize(test['sentences'], 'tokens'))
#pos = p.posdep(all)
results = pd.DataFrame()
for text in df['combined_text']:
pos = p.posdep(text)
pos_df = pd.json_normalize(pos['sentences'], 'tokens')
results = pd.concat([results,pos_df])
results.to_csv("annotated_trankit.csv")
View(results)
df.loc[:5]['combined_text']
test = df.loc[:5]['combined_text']
pd.json_normalize(pos['sentences'][1], 'tokens')
pd.json_normalize(pos['sentences'][0], 'tokens')
pd.json_normalize(pos['sentences'][0], 'id', 'text', 'tokens')
pd.json_normalize(pos['sentences'][0], 'id', *)
pd.json_normalize(pos['sentences'][0], 'id')
pd.json_normalize(pos['sentences'][0], 'id')
pd.json_normalize(pos['sentences'][0], 'text')
pd.json_normalize(pos['sentences'][0], 'tokens')
noun_data = results[results['upos'] == 'NOUN']
View(results)
top_nouns = noun_data['lemma'].value_counts().reset_index()
lemma = pd.DataFrame()
for text in df['combined_text']:
lemma = p.lemmatize(text)
lemma_df = pd.json_normalize(lemma['sentences'], 'tokens')
results_lemma = pd.concat([results_lemma,lemma_df])
results_lemma = pd.DataFrame()
for text in df['combined_text']:
lemma = p.lemmatize(text)
lemma_df = pd.json_normalize(lemma['sentences'], 'tokens')
results_lemma = pd.concat([results_lemma,lemma_df])
View(lemma_df)
noun_data = results_lemmma[results_lemma['upos'] == 'NOUN']
noun_data = results_lemma[results_lemma['upos'] == 'NOUN']
View(results_lemma)
results.to_csv("lemmas_trankit.csv")
results_lemma.to_csv("lemmas_trankit.csv")
results_complete = pd.concat([results, results_lemma['text']], axis=1)
View(results_complete)
results_complete = pd.concat([results, results_lemma['text'].rename("lemma")], axis=1)
results_complete["lemma"] = results_complete["text"]
View(results_complete)
results_complete.insert(2, "lemma", results_complete.pop("lemma"))
results_complete= results_complete.insert(2, "lemma", results_complete.pop("lemma"))
results_complete=[]
results_complete = pd.concat([results, results_lemma['text'].rename("lemma")], axis=1)
results_complete["lemma"] = results_complete["text"]
results_complete.to_csv("annotated_trankit.csv")
noun_data = results_complete[results_complete['upos'] == 'NOUN']
import pandas as pd
import plotly.express as px
# Charger les données à partir du DataFrame "results"
# Assurez-vous que "results" contient les mêmes colonnes que le fichier CSV original
# (par exemple, "upos" et "lemma")
# results = pd.read_csv("annotated_stanza.csv")
# Filtrer les lignes où 'upos' est égal à "NOUN"
noun_data = results_complete[results_complete['upos'] == 'NOUN']
# Regrouper par 'lemma' et compter le nombre d'occurrences
top_nouns = noun_data['lemma'].value_counts().reset_index()
top_nouns.columns = ['lemma', 'n']
top_nouns = top_nouns.head(20)
# Créer un graphique à l'aide de Plotly Express
fig = px.scatter(top_nouns, x='n', y='lemma', color='lemma',
labels={'n': 'Frequency', 'lemma': 'Lemma'},
title='Top 20 Most Frequent Nouns')
# Personnaliser le style du graphique
fig.update_traces(marker=dict(size=12, opacity=0.6),
selector=dict(mode='markers'))
fig.update_layout(title_x=0.5, title_font=dict(size=20))
# Afficher le graphique
fig.show()
fig.write_html("top_20_nouns_trankit_python.html")
