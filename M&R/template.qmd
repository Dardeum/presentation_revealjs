---
title: The diffusion of NLP methods in marketing research
#bibliography: references.bib
subtitle: A systematic analysis
format: 
  clean-revealjs:
    self-contained: true
author:
  - name: Olivier Caron
    orcid: 0000-0000-0000-0000
    email: olivier.caron@dauphine.psl.eu
    affiliations:
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    orcid: 0000-0002-7253-5747
    email: christophe.benavent@dauphine.psl.eu
    affiliations:
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date: 02/13/2024
date-format: long
editor: 
  markdown: 
    wrap: 72
bibliography: references.bib
csl: apa.csl
---

```{r}
#| label: libraries-data-r
#| include: false
#| eval: true

library(cowplot)
library(tidyverse)
library(ggstatsplot)
library(reticulate)
library(gt)
library(plotly)
library(countrycode)
library(htmlwidgets)
library(reactable)
library(wesanderson)
library(ggrepel)

scimago <- read.csv2("data/scimago_journals_marketing_onlyQ3_2022.csv")

list_articles <- read.csv2("data/marketing_Q1_to_Q3_scimago_journals_NLP.csv") %>%
  filter(year < 2024) %>%
  filter(entry_number != 769) %>% #remove the "NLP" article about neurolinguistic programming
  filter(!str_detect(subtypeDescription, "Proceedings|Conference|Transactions|Erratum|Letter|Note")) %>%
  mutate(
    authkeywords = str_replace_all(authkeywords, "\\|", ""), # Delete the "|" separator
    authkeywords = str_squish(authkeywords), # Let only one space between words and delete the space at the beginning and the end of the string
    combined_text = paste(dc_title, dc_description, authkeywords),
    combined_text = tolower(combined_text)) # lower so it will be easier to count words


#replace "." in the columns names by "_". But if there are multiple dots like "...", we replace only by one "_"
# if the dots are in the start or end of the column name, we remove them. Also replace multiple "_" by one "_"

colnames(list_articles) <- gsub("\\.+", ".", colnames(list_articles)) 
colnames(list_articles) <- gsub("^\\.|\\.$", "", colnames(list_articles)) 
colnames(list_articles) <- gsub("\\.", "_", colnames(list_articles)) 
colnames(list_articles) <- gsub("_+", "_", colnames(list_articles))

colnames(scimago) <- gsub("\\.+", ".", colnames(scimago))
colnames(scimago) <- gsub("^\\.|\\.$", "", colnames(scimago))
colnames(scimago) <- gsub("\\.", "_", colnames(scimago))
colnames(scimago) <- gsub("_+", "_", colnames(scimago))

write.csv2(list_articles, "data/filtered_articles.csv", row.names = FALSE)

#table(list_articles$subtypeDescription)
  

#list_references <- py$list_references %>%
  #mutate(year = strtoi(substr(`prism:coverDate`,1,4))) %>%
  #rename(scopus_eid = "scopus-eid",
         #authid = "author-list.author.@auid")

```

## Research context

### NLP methods are increasingly used in marketing research

NLP methods enable the conversion of text into quantifiable data for
in-depth analysis enabling the analysis of large volumes of text.

They are particularly suitable for marketing concerns:

-   Evaluate customer feebacks, gauging public sentiment
-   Detect emotional responses to products and marketing campaigns
-   Detect trends, consumer preferences and needs.

## Specific research practices

Institutional Framework promote competition in academia driven by
journal rankings and citation scores, influencing researcher
compensation and career advancement. [@richard2015]

Concentration of research in major publishers (Elsevier, Thomson) with
an increasing number of journals with facilitated access through
bibliographic search interfaces.

## What are the factors driving the diffusion of NLP methods? 1/2

Researcher must elaborate strategies to advance their career
[@kolesnikov2018]

-   Balance between productivity, impact, originality, legitimacy,
    learning costs.

These strategies can be thought of in terms of the technology acceptance
model from @davis1989

-   Adopt NLP methods based on interest (1) and ease of use (2)

::: notes
1.  Researchers are strategically adopting NLP methods to enhance their
    productivity and originality in marketing research

2.  infusion of NLP methods in marketing research driven by researchers'
    interest in innovative tools and the perceived ease of integrating
    these methods into their work.

3.  deliberate choice to balance research output (productivity) with the
    creation of impactful and original work within the marketing
    discipline.

4.  Researchers might lean towards NLP methods that offer a low barrier
    to entry and high potential for enhancing the visibility and
    legitimacy of their work.
:::

## What are the factors driving the diffusion of NLP methods? 2/2

NLP methods are used in close communities of practice [@lave1991;
@hauser2006]

Management trends [@abrahamson1991]

::: notes
1.  Community Practice: The adoption of NLP methods is fostered within
    tight-knit scholarly communities, suggesting that peer learning and
    shared practices play a crucial role in their diffusion.

2.  Diffusion linked to CULTURAL PHENOMENON RATHER THAN RATIONAL
    DECISION MAKING
:::

## Data presentation

### Articles

-   There are [`r n_distinct(list_articles$entry_number)`]{.fg
    style="--col: #e64173"} articles and
    [`r n_distinct(list_articles$authid)`]{.fg style="--col: #e64173"}
    different authors

-   Date of publication range from [`r min(list_articles$year)`]{.fg
    style="--col: #e64173"} to [`r max(list_articles$year)`]{.fg
    style="--col: #e64173"}

### Data collection

-   All data were collected from Scopus.

### Keywords

-   "natural language processing", "nlp", "embeddings", "chatgpt",
    "liwc", "transformers", "word2vec", "wordtovec", "lda", "text
    mining", "text-mining", "text analysis", "text analytics",
    "text-analytics", "text-analysis"

### Journals

-   [`r nrow(scimago)`]{.fg style="--col: #e64173"} journals ranked from
    the first to third quartile according to the [SCImago Journal
    Rank](https://www.scimagojr.com/journalrank.php?category=1406&area=1400&type=j "SCImago Marketing")
    in the field of marketing.

```{r}
#| label: worldwide-production
#| column: page
#| ncol: 2
#| include: false
#| eval: true

# regroup must productive affiliations by affiliation
# We count only one production per affiliation even if 3 authors from the same affiliation have published the same paper together
productive_affiliations <- list_articles %>%
  filter(!is.na(afid)) %>%
  group_by(entry_number,afid) %>%
  distinct(afid, .keep_all = TRUE) %>%
  ungroup() %>%
  group_by(afid, affilname, affiliation_city, affiliation_country) %>%
  reframe("number_productions" = n()) %>%
  arrange(desc(number_productions))



ggsave("images/25most_productive_affiliations.svg", width=10)
```

# General data vizualisation {background-color="#40666e"}

## Production per affiliation {.absolute}

::: {.absolute top="60" left="-120"}
```{r}
#| label: worldwide-production-affiliation
#| fig-height: 7
#| fig-width: 13

#have a look at countries on a worldmap
#specify some theme properties
plain <- theme(
  axis.text        = element_blank(),
  axis.line        = element_blank(),
  axis.ticks       = element_blank(),
  panel.border     = element_blank(),
  panel.grid       = element_blank(),
  axis.title       = element_blank(),
  panel.background = element_rect(fill = "white"),
  plot.title = element_text(hjust = 0.5)
)

#important to include the country name and the city name so that tidygeocoder can find the right city
#exemple : I had some problems with the city "Cambridge" which is in the US and in the UK. Now they both have different latitude and longitude
productive_affiliations$city_country <- paste(productive_affiliations$affiliation_city, productive_affiliations$affiliation_country, sep = ", ")

#enables us to get latitude and longitude of affiliation cities so we can place the cities on a worldmap
#It's a bit long so I commented it and saved the result in a csv file
#result_tidygeocoder <- tidygeocoder::geocode(productive_affiliations,
                                          #city_country,
                                          #method="osm") #OpenStreetMap data
                                          
#write_csv(result_tidygeocoder, "data/affiliations_geocoded.csv")

result_tidygeocoder <-read.csv("data/affiliations_geocoded.csv")

result_tidygeocoder <- result_tidygeocoder %>% filter(!is.na(affiliation_city))

mapWorld <- borders("world", colour="gray30", fill="#cbc8c3", size=0.2)
world <- map_data("world")
worldplot <- ggplot() +
  geom_polygon(data = world, aes(x = long, y = lat, group = group)) +
  geom_point(data = result_tidygeocoder, color="#f38181", alpha=0.9, aes(x=long, y = lat, size = number_productions))+
  geom_point(data = result_tidygeocoder,
             colour="black",
             shape=1,
             aes(x=long, y = lat,
                 size = number_productions,
                 text= paste0("Affiliation: ",
                              affilname,
                              "\nAffiliation city: ",
                              affiliation_city,
                              "\nNumber of productions: ",
                              number_productions),
                 stroke = 0.20))+ #stroke is the width of the second black point and shape=1 means we just want circles, not the full points
  mapWorld+
  plain+
  coord_map("equirectangular")+
  coord_cartesian(ylim = c(-50, 90))+ #get rid of Antarctica;
  labs(title="")+
  scale_size(range = c(1, 6))

worldplot_plotly_object <- ggplotly(worldplot, tooltip = "text")
worldplot_plotly_object %>% 
  config(scrollZoom = TRUE)


htmlwidgets::saveWidget(worldplot_plotly_object, "images/worldplot.html")
```
:::

## Citations per country {.absolute}

::: {.absolute top="70" left="-85"}
```{r}
#| label: worldwide-production-3D
#| fig-height: 7
#| fig-width: 13

cited_country <- list_articles %>%
  filter(!is.na(affiliation_country)) %>%
  group_by(entry_number,affiliation_country) %>%
  distinct(affiliation_country, .keep_all = TRUE) %>%
  ungroup() %>%
  group_by(affiliation_country) %>%
  reframe("number_citations" = sum(citedby_count)) %>%
  arrange(desc(number_citations))

cited_affiliation <- list_articles %>%
  filter(!is.na(affilname)) %>%
  group_by(entry_number,affilname) %>%
  distinct(affilname, .keep_all = TRUE) %>%
  ungroup() %>%
  group_by(affilname, affiliation_country, affiliation_city) %>%
  reframe("number_citations" = sum(citedby_count)) %>%
  arrange(desc(number_citations))#%>%


data3d <- cited_country %>% 
  mutate(code = countrycode::countrycode(sourcevar = affiliation_country,
                                 origin = "country.name",
                                 destination = "iso3c")) #we need three-letter country codes (ISO 3166-1 alpha-3) to pass into plot_geo, otherwise it doesn't work


#Set country boundaries as light grey
l <- list(color = toRGB("#d1d1d1"), width = 0.5)
#Specify map projection and options
g <- list(
           showframe      = TRUE,
           showcoastlines = FALSE,
           showsubunits   = TRUE,
           projection     = list(type = 'orthographic'), #globe
           resolution     = '100',
           showcountries  = TRUE,
           countrycolor   = '#d1d1d1',
           showocean      = TRUE,
           oceancolor     = '#c9d2e0',
           showlakes      = FALSE,
           lakecolor      = '#99c0db',
           showrivers     = FALSE,
           rivercolor     = '#99c0db',
           family         = "sans-serif",
           showlegend     = FALSE
          )


#worldmap with log(number of productions) 
p <- plot_geo(data3d) %>%
     add_trace(z = ~log(number_citations),
               color     = ~number_citations,
               colors    = 'OrRd',
               locations = ~code,
               text      = ~affiliation_country,
               showscale = FALSE,
               marker    = list(line = l)) %>%
     layout(title = "",
            geo = g,
            margin = list(l = 40, r = 10, b = 30, t = 30)) #%>% hide_colorbar() we can either specify showscale = FALSE at the trace level or pipe with hide_colorbar()
         
p

ggdotplotstats(
  data       = head(cited_country,25),
  y          = affiliation_country,
  x          = number_citations,
  test.value = 25,
  type       = "robust",
  title      = "Distribution of academic citations among the 25 most cited countries",
  xlab       = "Number of citations (articles, review, editorial)"
)

saveWidget(p,"images/3D_worldmap_citations.html")
```
:::

## A focus on affiliations: number of [productions]{.fg style="--col: #e64173"}

```{r}
#| label: ggstatplot-productions
#| fig-height: 6
#| fig-width: 11

ggdotplotstats(
  data       = head(productive_affiliations,25),
  y          = affilname,
  x          = number_productions,
  test.value = 25,
  type       = "robust",
  title      = "Distribution of academic productions among the 25 most productive affiliations",
  xlab       = "Number of productions (articles, review, editorial)"
)
```

## A focus on affiliations: number of [citations]{.fg style="--col: #3cb371"}

```{r}
#| label: ggstatplot-citations
#| fig-height: 6
#| fig-width: 11

ggdotplotstats(
  data       = head(cited_affiliation,25),
  y          = affilname,
  x          = number_citations,
  test.value = 25,
  type       = "robust",
  title      = "Distribution of academic citations among the 25 most cited affiliations",
  xlab       = "Number of citations (articles, review, editorial)"
)
```

## Management trend in publication volume

```{r}
#| label: evolution-publications
#| fig-height: 12
#| fig-width: 20
#| fig-align: center
#| column: page

nlp_papers <- list_articles

#get rid of conference papers
#nlp_papers_journal_only <- nlp_papers %>%
  #filter(!grepl("conference", subtypeDescription, ignore.case = TRUE)) #%>%
  #filter(year < 2023)


t0 <-prop.table(table(nlp_papers$`prism_publicationName`)) %>%
  as.data.frame() %>%
  arrange(desc(Freq)) %>%
  mutate(Var1 = sub(".*:", "", Var1)) %>% #just because title of Proceedings of the Academy... is too long to display
  head(30)

t0 <- left_join(t0, scimago, by = c(Var1 = "Title"))

t1<-as.data.frame(table(nlp_papers$year)) 

g01 <- ggplot(t0, aes(x=reorder(paste(Var1, "-", SJR_Quartile, "| Rank:", Rank), Freq), y=Freq)) +
  geom_bar(stat="identity", fill="steelblue") +
  coord_flip() +
  theme_minimal(base_size = 12) + 
  labs(title="Number of Articles per Journal", y="Proportion", x="") +
  theme(
    axis.title.x = element_text(size = 16),
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),  
    axis.text.y = element_text(size = 10),
    plot.title = element_text(size = 16)
  )


# Graph 2: Number of publications per year
g02 <- ggplot(t1, aes(x=Var1, y=Freq, group=1)) +
  geom_line(size=1.1, color="steelblue") +
  geom_point(size=2, color="steelblue") +
  geom_smooth(color="#7D7C7C", linewidth=0.5)+
  theme_minimal() +
  labs(title="Number of Publications per Year", y="", x="Year") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
    axis.title.x = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16) # Set title size to 16
  )


plotgrid <- plot_grid(g01,
                      g02,
                      label_size = 10,
                      ncol=2,
                      rel_widths =  c(1,1))
ggsave(filename="images/evolution_publications_nlp_marketing.png",
       width = 80, 
       height = 40, 
       units = "cm")
plotgrid
```

# Networks & Communities {background-iframe="colored-particles/index.html"}

## The structure of author networks ([until 2015](https://oliviercaron.github.io/systematic_lit_review/networks/authors/1996_2015_VF.html){target="_blank"})

![](data/networks/1996-2015-auteurs.png){width="100%"}

## The structure of author networks ([until 2023](https://oliviercaron.github.io/systematic_lit_review/networks/authors/1996_2023_VF.html){target="_blank"})

![](data/networks/1996-2023-auteurs_node_size_large.png){width="100%"}

## Some measures of network structure

![](images/density_networks.png){fig-align="center"}

::: notes
1.  Network growth is outpacing the formation of new collaborations,
    leading to a less densely connected community.

2.  The increasing number of communities reflects a diversification in
    research subjects and group formations.

3.  A surge in collaborations suggests a trend towards more collective
    research efforts in NLP.

4.  The expanding average size of research groups per community
    indicates the formation of larger, more collaborative teams.
:::

## The unequal spread of citations between communities

The top 5 communities account for [13.65% of authors]{.fg
style="--col: #e64173"} (126) and [46% of citations]{.fg
style="--col: #e64173"} (14851)

![](images/pareto_chart_ggplot_no_xtickslabels.png){fig-align="center"}

# Topics {background-color="#40666e"}

## Topic modeling with STM

-   STM algorithm with years as covariates [@roberts2014]

::: notes
what we can understand from the result of these topics is that

1.  words like "customer", "reviews" and "sentiment" : strong focus on
    consumer perceptions.

2.  words related to service quality : improve service offerings.

3.  terms like "brand," "strategy," and "engagement" appearing :
    strategic brand positioning.

4.  "social media" related terms indicates that platforms are key
    sources for data mining, providing rich insights for market trends
    and consumer behavior analysis.

5.  "segmentation" with analytical terms shows an inclination towards
    using NLP for slicing the market into well-defined segments.
:::

![](images/stm_topic16.jpg){fig-align="center" width="120%"}

## A concentric diffusion of topics

::: notes
1.  The number of topics per period increases steadily without the
    relative number of articles per topic decreasing, in a process of
    accumulation rather than substitution.
:::

::: r-stack
![](images/stm_effect16.jpg){fig-align="center" width="85%"
height="70%"}

![](images/stm_period.png){.fragment fig-align="center" width="90%"}
:::

# A focus on methods {background-color="#40666e"}

## Main NLP techniques used

-   "O" or "1" if a technique is detected in the articles

-   combined text of abstract, title, and keywords

-   detection of the technique name and the different spellings

```{r}
#| label: show-code
#| echo: true
#| eval: false

bert_alt <- c(
  "bert",
  "bi-directional encoder representation from transformers",
  "bi-directional encoder representations from transformer",
  "bi-directional encoder representations from transformers",
  "bidirectional encoder representation from transformer",
  "bidirectional encoder representation from transformers",
  "bidirectional encoder representations from transformer",
  "bidirectional encoder representations from transformers",
  "bi directional encoder representation from transformer",
  "bi directional encoder representation from transformers",
  "bi directional encoder representations from transformer",
  "bi directional encoder representations from transformers"
)
```

```{r}
#| label: count-nlp-techniques

list_articles_techniques <- list_articles %>%
  distinct(entry_number, .keep_all = TRUE)

## NLP TECHNIQUES

liwc_alt <- c(
  "liwc",
  "linguistic inquiry and word count",
  "linguistic inquiry & word count",
  "linguistic inquiry word count",
  "linguistic word count"
)

chatgpt_alt <- c(
  "chatgpt", "chat gpts",
  "chat gpt", "chat gpts",
  "chat generative pre-trained transformer", "chat generative pre-trained transformers",
  "chat generative pretrained transformer", "chat generative pretrained transformers"
)
bert_alt <- c(
  "bert",
  "bi-directional encoder representation from transformer",
  "bi-directional encoder representation from transformers",
  "bi-directional encoder representations from transformer",
  "bi-directional encoder representations from transformers",
  "bidirectional encoder representation from transformer",
  "bidirectional encoder representation from transformers",
  "bidirectional encoder representations from transformer",
  "bidirectional encoder representations from transformers",
  "bi directional encoder representation from transformer",
  "bi directional encoder representation from transformers",
  "bi directional encoder representations from transformer",
  "bi directional encoder representations from transformers"
)

llm_alt <- c(
  "large language model", "large language models",
  "large scale language model", "large scale language models"
)

lda_alt <- c(
  "latent dirichlet allocation", "latent dirichlet allocations",
  "lda", "ldas",
  "dirichlet allocation", "dirichlet allocations"
)

stm_alt <- c(
  "structural topic model", "structural topic models",
  "structural topic modeling", "structural topic modelings",
  "structural topic modelling", "structural topic modellings"
)

ner_alt <- c(
  "named entity recognition", "named entity recognitions",
  "entity recognition", "entity recognitions"
)

sentiment_analysis_alt <- c(
  "sentiment analysis"
)

topic_modeling_alt <- c(
  "topic modeling", "topic modelings",
  "topic model", "topic models",
  "topic modelling", "topic modellings"
)

tfidf_alt <- c(
  "tf-idf", "tf-idfs",
  "tfidf", "tfidfs",
  "term frequency-inverse document frequency", "term frequency-inverse document frequencies",
  "term frequency inverse document frequency", "term frequency inverse document frequencies"
)

embeddings_alt <- c(
  "word embedding", "word embeddings",
  "embeddings"
)

transformers_alt <- c(
  "transformers",
  "transformers model", "transformer models"
)

roberta_alt <- c(
  "roberta", "robertas",
  "robustly optimized bert approach", "robustly optimized bert approaches"
)

word2vec_alt <- c(
  "word2vec", "word2vecs",
  "word to vec", "word to vecs",
  "word 2 vec", "word 2 vecs",
  "word2vec model", "word2vec models",
  "word to vectors", "word to vector",
  "word 2 vectors", "word 2 vector"
)

fasttext_alt <- c(
  "fasttext", "fasttexts"
)

textrank_alt <- c(
  "textrank", "textranks"
)

gpt2_alt <- c(
  "gpt2", "gpt-2", "gpt 2",
  "generative pre-trained transformer 2", "generative pre-trained transformers 2",
  "generative pretrained transformer 2", "generative pretrained transformers 2"
)

gpt3_alt <- c(
  "gpt3", "gpt-3", "gpt 3",
  "generative pre-trained transformer 3", "generative pre-trained transformers 3",
  "generative pretrained transformer 3", "generative pretrained transformers 3"
)

pos_tagging_alt <- c(
  "pos tagging", "pos taggings",
  "part of speech tagging", "parts of speech tagging",
  "pos tagger", "pos taggers"
)

para_alt <- c(
  "paralanguage classifier", "paralanguage classifiers",
  "textual paralanguage classifier", "textual paralanguage classifiers"
)

leximancer_alt <- c(
  "leximancer"
)

passivepy_alt <- c(
  "passivepy"
)

# count the presence of each technique in the abstracts, title, and keywords
check_presence <- function(text, keywords) {
  sapply(keywords, function(keyword) grepl(keyword, text, ignore.case = TRUE)) %>% any()
}

# Add columns for each technique based on above dictionaries
techniques <- list(LIWC = liwc_alt, ChatGPT = chatgpt_alt, LLM = llm_alt, LDA = lda_alt, STM = stm_alt, 
                   BERT = bert_alt, NER = ner_alt, Sentiment_Analysis = sentiment_analysis_alt, 
                   TFIDF = tfidf_alt, Embeddings = embeddings_alt, Transformers = transformers_alt, 
                   RoBERTa = roberta_alt, Word2Vec = word2vec_alt, FastText = fasttext_alt, 
                   TextRank = textrank_alt, GPT2 = gpt2_alt, GPT3 = gpt3_alt, POS_Tagging = pos_tagging_alt,
                   PARA = para_alt, Leximancer = leximancer_alt, PassivePy = passivepy_alt)

# create a column for each technique to which we apply the check_presence function to populate the column with 1s and 0s
for (technique_name in names(techniques)) {
  list_articles_techniques[[technique_name]] <- sapply(list_articles_techniques$combined_text, check_presence, techniques[[technique_name]])
  list_articles_techniques[[technique_name]] <- as.integer(list_articles_techniques[[technique_name]])
}

# number of techniques per year
techniques_per_year <- list_articles_techniques %>%
  group_by(year) %>%
  summarise(across(all_of(names(techniques)), sum, na.rm = TRUE)) %>%
  ungroup()

# cumulative number of techniques per year
cumulative_techniques_year <- techniques_per_year %>%
  arrange(year) %>%
  mutate(across(-year, cumsum))

techniques_2023 <- cumulative_techniques_year %>%
  filter(year == 2023) %>% 
  pivot_longer(-year, names_to = "technique", values_to = "count") %>%
  select(-year) %>%
  arrange(desc(count))

top_1O_techniques <- techniques_2023$technique  %>%
  head(10)

# Sum the number of techniques used in each article
list_articles_techniques$sum_techniques <- rowSums(list_articles_techniques[names(techniques)])
most_technical_articles <- list_articles_techniques[order(list_articles_techniques$sum_techniques, decreasing = TRUE),] %>%
  select(entry_number, dc_title, 23, year, sum_techniques, Rank, SJR, citedby_count)

```
```{r}
#| label: first-use-techniques

first_use <- data.frame(technique = character(), year = integer())

# Loop through each column starting from the 2nd column
for (col in 2:ncol(techniques_per_year)) {
  # Find the first row where the value is greater than 0
  first_row <- which(techniques_per_year[[col]] > 0)[1]
  
  # Check if there is at least one value greater than 0
  if (length(first_row) > 0) {
    # Get the year from the first column for this row
    year <- techniques_per_year$year[first_row]
    
    # Append to the first_use dataframe
    first_use <- rbind(first_use, data.frame(technique = names(techniques_per_year)[col], year_first_use = year))
  }
}

first_use <- first_use %>%
  arrange(desc(year_first_use))

```

## Delay in the use of techniques

```{r}
#| label: first-use-techniques-fill
#| fig-height: 7
#| fig-width: 13

first_use$year_publication[first_use$technique == "ChatGPT"] <- "2022"
first_use$year_publication[first_use$technique == "LLM"] <- ""
first_use$year_publication[first_use$technique == "LDA"] <- "2003"
first_use$year_publication[first_use$technique == "STM"] <- "2014"
first_use$year_publication[first_use$technique == "BERT"] <- "2018"
first_use$year_publication[first_use$technique == "NER"] <- ""
first_use$year_publication[first_use$technique == "Sentiment_Analysis"] <- ""
first_use$year_publication[first_use$technique == "TFIDF"] <- "test"
first_use$year_publication[first_use$technique == "Embeddings"] <- "2013"
first_use$year_publication[first_use$technique == "Transformers"] <- "2017"
first_use$year_publication[first_use$technique == "RoBERTa"] <- "2019"
first_use$year_publication[first_use$technique == "Word2Vec"] <- "2013"
first_use$year_publication[first_use$technique == "FastText"] <- "2015"
first_use$year_publication[first_use$technique == "TextRank"] <- "2004"
first_use$year_publication[first_use$technique == "GPT2"] <- "2019"
first_use$year_publication[first_use$technique == "GPT3"] <- "2020"
first_use$year_publication[first_use$technique == "POS_Tagging"] <- ""
first_use$year_publication[first_use$technique == "PARA"] <- "2023"
first_use$year_publication[first_use$technique == "Leximancer"] <- ""
first_use$year_publication[first_use$technique == "PassivePy"] <- "2023"

first_use <- first_use %>%
  filter(year_publication != "") %>%
  mutate(year_publication = as.integer(year_publication),
         year_first_use = as.integer(year_first_use),
         delay  = year_first_use - year_publication,
         technique_year = paste(technique,"-",year_publication)) %>%
  arrange(year_publication)

techniques_lolipop <- ggplot(first_use, aes(x = reorder(technique_year, year_publication), y = year_first_use)) +
  geom_segment(aes(xend = technique_year, yend = year_publication), colour = "gray") +
  geom_point(aes(y = year_first_use), colour = "#ff2c2c", size = 3) +
  geom_point(aes(y = year_publication), colour = "skyblue", size = 3) +
  geom_text(aes(x = technique_year, y = (year_first_use + year_publication) / 2, label = delay), vjust = -0.5) +
  coord_flip() +
  labs(y = "Year", x = "", title = "Timeline of Techniques") +
  scale_x_discrete(limits = rev(first_use$technique_year)) +
  theme_minimal()

techniques_lolipop

#ggplotly(techniques_lolipop, tooltip = c("technique", "year_publication", "year_first_use", "delay"))

ggsave("images/first_use_techniques.png", width = 10, height = 6, dpi = 600)

```


## Evolution of NLP techniques

```{r}
#| label: graph-techniques-table

techniques_2023 %>%
  pivot_wider(names_from = technique, values_from = count) %>%
  gt() %>%
  tab_options(
    column_labels.font.weight = "bold"
  )
  
```

```{r}
#| label: graph-techniques-plotly

#other technique to plot with label on curves 

Royal2 <- wesanderson::wes_palette("GrandBudapest1", n = 10, type = "continuous")

fig2 <- ggplot(cumulative_techniques_year, aes(x = year)) +
  geom_line(aes(y = Sentiment_Analysis, color = "Sentiment Analysis")) +
  geom_line(aes(y = LDA, color = "LDA")) +
  geom_line(aes(y = LIWC, color = "LIWC")) +
  geom_line(aes(y = ChatGPT, color = "ChatGPT")) +
  geom_line(aes(y = Embeddings, color = "Embeddings")) +
  geom_line(aes(y = Transformers, color = "Transformers")) +
  geom_line(aes(y = Leximancer, color = "Leximancer")) +
  geom_line(aes(y = BERT, color = "BERT")) +
  geom_line(aes(y = STM, color = "STM")) +
  geom_line(aes(y = Word2Vec, color = "Word2Vec")) +
  scale_color_manual(values = Royal2) +
  xlim(2010, 2024) +
  labs(
    title = "Evolution of NLP Techniques (Top 10 by 2023)",
    subtitle = "Cumulative sum of articles mentioning each technique",
    x = "Year",
    y = "Cumulative Count"
  ) +
  theme_minimal() +
  theme(legend.position = "none") # Hide the legend because the labels are directly on the curves

#fig2 <- fig2 +
  #geom_point(aes(y = Sentiment_Analysis), color="black", size=0.75) +
  #geom_point(aes(y = LDA), color="black", size=0.75) +
  #geom_point(aes(y = LIWC), color="black", size=0.75) +
  #geom_point(aes(y = ChatGPT), color="black", size=0.75) +
  #geom_point(aes(y = Embeddings), color="black", size=0.75) +
  #geom_point(aes(y = Transformers), color="black", size=0.75) +
  #geom_point(aes(y = Leximancer), color="black", size=0.75) +
  #geom_point(aes(y = BERT), color="black", size=0.75) +
  #geom_point(aes(y = STM), color="black", size=0.75) +
  #geom_point(aes(y = Word2Vec), color="black", size=0.75)
  

# Add labels at the end of each curve with geom_text_repel
labels_positions <- data.frame(
  year = 2023, # reference year for positioning labels
  Sentiment_Analysis = cumulative_techniques_year$Sentiment_Analysis[cumulative_techniques_year$year == 2023],
  LDA = cumulative_techniques_year$LDA[cumulative_techniques_year$year == 2023],
  LIWC = cumulative_techniques_year$LIWC[cumulative_techniques_year$year == 2023],
  ChatGPT = cumulative_techniques_year$ChatGPT[cumulative_techniques_year$year == 2023],
  Embeddings = cumulative_techniques_year$Embeddings[cumulative_techniques_year$year == 2023],
  Transformers = cumulative_techniques_year$Transformers[cumulative_techniques_year$year == 2023],
  Leximancer = cumulative_techniques_year$Leximancer[cumulative_techniques_year$year == 2023],
  BERT = cumulative_techniques_year$BERT[cumulative_techniques_year$year == 2023],
  STM = cumulative_techniques_year$STM[cumulative_techniques_year$year == 2023],
  Word2Vec = cumulative_techniques_year$Word2Vec[cumulative_techniques_year$year == 2023]
)

fig_plotly <- fig2 #to use without geom_text_repel but geom_text because not supported yet by plotly
for(technique in names(labels_positions)[-1]) {
  fig_plotly <- fig_plotly + geom_text(data = labels_positions, 
                               aes_string(x = "year", y = technique, label = gsub("_", " ", sprintf("'%s'", technique))),
                               nudge_x = 0.5, nudge_y = 0)
}
ggplotly(fig_plotly)

```

```{r}
#| label: graph-techniques-ggplot
#| eval: false
#| echo: false
# For each technique, add a label using geom_text_repel
for(technique in names(labels_positions)[-1]) {
  fig2 <- fig2 + geom_text_repel(data = labels_positions, 
                               aes_string(x = "year", y = technique, gsub("_", " ", sprintf("'%s'", technique))),
                               nudge_x = 0.5, nudge_y = 0, force = 10)
}
fig2
```

## Diffusion and delay of NLP techniques

There is a significant time lag between the emergence of techniques and
their actual use in marketing research:

-   LIWC: 1993 -\> 2014
-   LDA: 2003 -\> 2010

which is decreasing over time:

-   Embeddings: 2013 -\> 2017

-   BERT: 2018 -\> 2021

-   ChatGPT: 2022 -\> 2023

## Dominantly marketing: the selective borrowing of NLP methods

<iframe width="1200" height="1000" src="images/top20ref.html" title="Sigma graph" frameborder="0" class="column-page">

</iframe>

## To conclude:

1.  NLP methods' adoption in top-tier marketing journals suggests it's
    not a mere trend but an innovation gaining solid ground in the
    field.
2.  The cost of learning NLP is decreasing over time, facilitated by a
    growing repository of shared knowledge.
3.  The integration and diffusion of NLP techniques are bolstered within
    communities of practice.
4.  Diffusion is driven by new research questions, data availability,
    and suitable techniques.
5.  Delay between the introduction of a technique and its adoption is
    decreasing over time.

## References

::: {#refs}
:::

## Appendix
