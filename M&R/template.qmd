---
title: The diffusion of NLP methods in marketing research
#bibliography: references.bib
subtitle: A systematic analysis
format: 
  clean-revealjs
author:
  - name: Olivier Caron
    orcid: 0000-0000-0000-0000
    email: olivier.caron@dauphine.psl.eu
    affiliations:
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
  - name: Christophe Benavent
    orcid: 0000-0002-7253-5747
    email: christophe.benavent@dauphine.psl.eu
    affiliations:
      name: "Paris Dauphine - PSL"
      city: Paris
      state: France
date: last-modified
editor: 
  markdown: 
    wrap: 72
bibliography: references.bib
csl: apa.csl
---

```{r}
#| label: libraries-data-r
#| include: false
#| eval: true

library(cowplot)
library(tidyverse)
library(ggstatsplot)
library(reticulate)
library(gt)
library(plotly)
library(countrycode)
library(htmlwidgets)
library(reactable)
library(wesanderson)
library(ggrepel)

scimago <- read.csv2("data/scimago_journals_marketing_onlyQ3_2022.csv")

list_articles <- read.csv2("data/marketing_Q1_to_Q3_scimago_journals_NLP.csv") %>%
  filter(year < 2024) %>%
  filter(entry_number != 769) %>% #remove the "NLP" article about neurolinguistic programming
  filter(!str_detect(subtypeDescription, "Proceedings|Conference|Transactions|Erratum|Letter|Note")) %>%
  mutate(
    authkeywords = str_replace_all(authkeywords, "\\|", ""), # Delete the "|" separator
    authkeywords = str_squish(authkeywords), # Let only one space between words and delete the space at the beginning and the end of the string
    combined_text = paste(dc_title, dc_description, authkeywords),
    combined_text = tolower(combined_text)) # lower so it will be easier to count words


#replace "." in the columns names by "_". But if there are multiple dots like "...", we replace only by one "_"
# if the dots are in the start or end of the column name, we remove them. Also replace multiple "_" by one "_"

colnames(list_articles) <- gsub("\\.+", ".", colnames(list_articles)) 
colnames(list_articles) <- gsub("^\\.|\\.$", "", colnames(list_articles)) 
colnames(list_articles) <- gsub("\\.", "_", colnames(list_articles)) 
colnames(list_articles) <- gsub("_+", "_", colnames(list_articles))

colnames(scimago) <- gsub("\\.+", ".", colnames(scimago))
colnames(scimago) <- gsub("^\\.|\\.$", "", colnames(scimago))
colnames(scimago) <- gsub("\\.", "_", colnames(scimago))
colnames(scimago) <- gsub("_+", "_", colnames(scimago))

write.csv2(list_articles, "data/filtered_articles.csv", row.names = FALSE)

#table(list_articles$subtypeDescription)
  

#list_references <- py$list_references %>%
  #mutate(year = strtoi(substr(`prism:coverDate`,1,4))) %>%
  #rename(scopus_eid = "scopus-eid",
         #authid = "author-list.author.@auid")

```

```{python}
#| label: libraries-data-python
#| include: false
#| eval: true
import pandas as pd

#list_articles = pd.read_csv("data/filtered_articles", sep=';', decimal=',')

#list_articles = pd.read_csv("data/marketing_Q1_to_Q3_scimago_journals_NLP.csv", sep=';', decimal=',')
#list_articles = list_articles.loc[list_articles["year"] < 2024]

#list_articles = list_articles[list_articles['marketing'] == 1] #only marketing articles
#list_references = pd.read_csv("data/nlp_references_final_18-08-2023.csv", sep=';', decimal=',')

```

## Research context

### NLP methods are increasingly used in marketing research

-   NLP methods enable the conversion of text into quantifiable data for
    in-depth analysis.

-   These methods make it possible to evaluate a very large amount of
    data, which is often impossible or incomplete with qualitative
    methods.

-   They are particularly suitable for marketing concerns:

    -   Evaluate customer feebacks, gauging public sentiment
    -   Detect emotional responses to products and marketing campaigns
    -   Detect trends, consumer preferences and needs.

## Specific research practices

-   Institutional Framework promote competition in academia driven by
    journal rankings and citation scores, influencing researcher
    compensation and career advancement. [@richard2015]

-   Concentration of research in major publishers (Elsevier, Thomson)
    with an increasing number of journals with facilitated access
    through bibliographic search interfaces.

## What are the factors driving the diffusion of NLP methods?

-   Researcher must elaborate strategies to advance their career
    [@kolesnikov2018]

    -   Balance between productivity, impact, originality, legitimacy,
        learning costs.

-   These strategies can be thought of in terms of the technology
    acceptance model from @davis1989

    -   Adopt NLP methods based on interest (1) and ease of use (2)

-   NLP methods are used in close communities of practice [@lave1991;
    @hauser2006]

-   Management trends [@abrahamson1991]

## Data presentation

### Articles

-   There are [`r n_distinct(list_articles$entry_number)`]{.fg
    style="--col: #e64173"} articles and
    [`r n_distinct(list_articles$authid)`]{.fg style="--col: #e64173"}
    unique authors

-   Date of publication range from [`r min(list_articles$year)`]{.fg
    style="--col: #e64173"} to [`r max(list_articles$year)`]{.fg
    style="--col: #e64173"}

### Data collection

-   All data were collected from Scopus.

### Keywords

-   "natural language processing", "nlp", "embeddings", "chatgpt",
    "liwc", "transformers", "word2vec", "wordtovec", "lda", "text
    mining", "text-mining", "text analysis", "text analytics",
    "text-analytics", "text-analysis"

### Journals

-   [`r nrow(scimago)`]{.fg style="--col: #e64173"} journals ranked from
    the first to third quartile according to the [SCImago Journal
    Rank](https://www.scimagojr.com/journalrank.php?category=1406&area=1400&type=j "SCImago Marketing")
    in the field of marketing.

```{r}
#| label: worldwide-production
#| column: page
#| ncol: 2
#| include: false
#| eval: true

# regroup must productive affiliations by affiliation
# We count only one production per affiliation even if 3 authors from the same affiliation have published the same paper together
productive_affiliations <- list_articles %>%
  filter(!is.na(afid)) %>%
  group_by(entry_number,afid) %>%
  distinct(afid, .keep_all = TRUE) %>%
  ungroup() %>%
  group_by(afid, affilname, affiliation_city, affiliation_country) %>%
  reframe("number_productions" = n()) %>%
  arrange(desc(number_productions))



ggsave("images/25most_productive_affiliations.svg", width=10)
```

## Production per affiliation {.absolute}

::: {.absolute top="60" left="-120"}
```{r}
#| label: worldwide-production-affiliation
#| fig-height: 7
#| fig-width: 13

#have a look at countries on a worldmap
#specify some theme properties
plain <- theme(
  axis.text        = element_blank(),
  axis.line        = element_blank(),
  axis.ticks       = element_blank(),
  panel.border     = element_blank(),
  panel.grid       = element_blank(),
  axis.title       = element_blank(),
  panel.background = element_rect(fill = "white"),
  plot.title = element_text(hjust = 0.5)
)

#important to include the country name and the city name so that tidygeocoder can find the right city
#exemple : I had some problems with the city "Cambridge" which is in the US and in the UK. Now they both have different latitude and longitude
productive_affiliations$city_country <- paste(productive_affiliations$affiliation_city, productive_affiliations$affiliation_country, sep = ", ")

#enables us to get latitude and longitude of affiliation cities so we can place the cities on a worldmap
#It's a bit long so I commented it and saved the result in a csv file
#result_tidygeocoder <- tidygeocoder::geocode(productive_affiliations,
                                          #city_country,
                                          #method="osm") #OpenStreetMap data
                                          
#write_csv(result_tidygeocoder, "data/affiliations_geocoded.csv")

result_tidygeocoder <-read.csv("data/affiliations_geocoded.csv")

result_tidygeocoder <- result_tidygeocoder %>% filter(!is.na(affiliation_city))

mapWorld <- borders("world", colour="gray30", fill="#cbc8c3", size=0.2)
world <- map_data("world")
worldplot <- ggplot() +
  geom_polygon(data = world, aes(x = long, y = lat, group = group)) +
  geom_point(data = result_tidygeocoder, color="#f38181", alpha=0.9, aes(x=long, y = lat, size = number_productions))+
  geom_point(data = result_tidygeocoder,
             colour="black",
             shape=1,
             aes(x=long, y = lat,
                 size = number_productions,
                 text= paste0("Affiliation: ",
                              affilname,
                              "\nAffiliation city: ",
                              affiliation_city,
                              "\nNumber of productions: ",
                              number_productions),
                 stroke = 0.20))+ #stroke is the width of the second black point and shape=1 means we just want circles, not the full points
  mapWorld+
  plain+
  coord_map("equirectangular")+
  coord_cartesian(ylim = c(-50, 90))+ #get rid of Antarctica;
  labs(title="")+
  scale_size(range = c(1, 6))

worldplot_plotly_object <- ggplotly(worldplot, tooltip = "text")
worldplot_plotly_object %>% 
  config(scrollZoom = TRUE)


htmlwidgets::saveWidget(worldplot_plotly_object, "images/worldplot.html")
```
:::

## Citations per country {.absolute}

::: {.absolute top="70" left="-85"}
```{r}
#| label: worldwide-production-3D
#| fig-height: 7
#| fig-width: 13

cited_country <- list_articles %>%
  filter(!is.na(affiliation_country)) %>%
  group_by(entry_number,affiliation_country) %>%
  distinct(affiliation_country, .keep_all = TRUE) %>%
  ungroup() %>%
  group_by(affiliation_country) %>%
  reframe("number_citations" = sum(citedby_count)) %>%
  arrange(desc(number_citations))

cited_affiliation <- list_articles %>%
  filter(!is.na(affilname)) %>%
  group_by(entry_number,affilname) %>%
  distinct(affilname, .keep_all = TRUE) %>%
  ungroup() %>%
  group_by(affilname, affiliation_country, affiliation_city) %>%
  reframe("number_citations" = sum(citedby_count)) %>%
  arrange(desc(number_citations))#%>%


data3d <- cited_country %>% 
  mutate(code = countrycode::countrycode(sourcevar = affiliation_country,
                                 origin = "country.name",
                                 destination = "iso3c")) #we need three-letter country codes (ISO 3166-1 alpha-3) to pass into plot_geo, otherwise it doesn't work


#Set country boundaries as light grey
l <- list(color = toRGB("#d1d1d1"), width = 0.5)
#Specify map projection and options
g <- list(
           showframe      = TRUE,
           showcoastlines = FALSE,
           showsubunits   = TRUE,
           projection     = list(type = 'orthographic'), #globe
           resolution     = '100',
           showcountries  = TRUE,
           countrycolor   = '#d1d1d1',
           showocean      = TRUE,
           oceancolor     = '#c9d2e0',
           showlakes      = FALSE,
           lakecolor      = '#99c0db',
           showrivers     = FALSE,
           rivercolor     = '#99c0db',
           family         = "sans-serif",
           showlegend     = FALSE
          )


#worldmap with log(number of productions) 
p <- plot_geo(data3d) %>%
     add_trace(z = ~log(number_citations),
               color     = ~number_citations,
               colors    = 'OrRd',
               locations = ~code,
               text      = ~affiliation_country,
               showscale = FALSE,
               marker    = list(line = l)) %>%
     layout(title = "",
            geo = g,
            margin = list(l = 40, r = 10, b = 30, t = 30)) #%>% hide_colorbar() we can either specify showscale = FALSE at the trace level or pipe with hide_colorbar()
         
p

ggdotplotstats(
  data       = head(cited_country,25),
  y          = affiliation_country,
  x          = number_citations,
  test.value = 25,
  type       = "robust",
  title      = "Distribution of academic citations among the 25 most cited countries",
  xlab       = "Number of citations (articles, review, editorial)"
)

saveWidget(p,"images/3D_worldmap_citations.html")
```
:::

## A focus on affiliations: number of [productions]{.fg style="--col: #e64173"}

```{r}
#| label: ggstatplot-productions
#| fig-height: 6
#| fig-width: 11

ggdotplotstats(
  data       = head(productive_affiliations,25),
  y          = affilname,
  x          = number_productions,
  test.value = 25,
  type       = "robust",
  title      = "Distribution of academic productions among the 25 most productive affiliations",
  xlab       = "Number of productions (articles, review, editorial)"
)
```

## A focus on affiliations: number of [citations]{.fg style="--col: #3cb371"}

```{r}
#| label: ggstatplot-citations
#| fig-height: 6
#| fig-width: 11

ggdotplotstats(
  data       = head(cited_affiliation,25),
  y          = affilname,
  x          = number_citations,
  test.value = 25,
  type       = "robust",
  title      = "Distribution of academic citations among the 25 most cited affiliations",
  xlab       = "Number of citations (articles, review, editorial)"
)
```

## Management trend in publication volume

```{r}
#| label: evolution-publications
#| fig-height: 12
#| fig-width: 20
#| fig-align: center
#| column: page

nlp_papers <- list_articles

#get rid of conference papers
#nlp_papers_journal_only <- nlp_papers %>%
  #filter(!grepl("conference", subtypeDescription, ignore.case = TRUE)) #%>%
  #filter(year < 2023)


t0 <-prop.table(table(nlp_papers$`prism_publicationName`)) %>%
  as.data.frame() %>%
  arrange(desc(Freq)) %>%
  mutate(Var1 = sub(".*:", "", Var1)) %>% #just because title of Proceedings of the Academy... is too long to display
  head(30)

t0 <- left_join(t0, scimago, by = c(Var1 = "Title"))

t1<-as.data.frame(table(nlp_papers$year)) 

g01 <- ggplot(t0, aes(x=reorder(paste(Var1, "-", SJR_Quartile, "| Rank:", Rank), Freq), y=Freq)) +
  geom_bar(stat="identity", fill="steelblue") +
  coord_flip() +
  theme_minimal(base_size = 12) + 
  labs(title="Number of Articles per Journal", y="Proportion", x="") +
  theme(
    axis.title.x = element_text(size = 16),
    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),  
    axis.text.y = element_text(size = 10),
    plot.title = element_text(size = 16)
  )


# Graph 2: Number of publications per year
g02 <- ggplot(t1, aes(x=Var1, y=Freq, group=1)) +
  geom_line(size=1.1, color="steelblue") +
  geom_point(size=2, color="steelblue") +
  geom_smooth(color="#7D7C7C", linewidth=0.5)+
  theme_minimal() +
  labs(title="Number of Publications per Year", y="", x="Year") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
    axis.title.x = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16) # Set title size to 16
  )


plotgrid <- plot_grid(g01,
                      g02,
                      label_size = 10,
                      ncol=2,
                      rel_widths =  c(1,1))
ggsave(filename="images/evolution_publications_nlp_marketing.png",
       width = 80, 
       height = 40, 
       units = "cm")
plotgrid
```

# The network of authors is becoming increasingly sparse {background-iframe="colored-particles/index.html"}

## The structure of author networks (until 2015)

## The structure of author networks (until 2023)

## Some measures of network structure

## A concentric propagation of topics

-   STM algorithm with year as covariates

## Main NLP techniques used

We want to count the number of times each NLP technique is used in the articles.
To do so, we will detect the present of the name of the techniques and their different synonyms in the abstracts, title, and keywords of the articles.
If it appears multiple times, we will count it only once.


```{r}
#| label: count-nlp-techniques

list_articles_techniques <- list_articles %>%
  distinct(entry_number, .keep_all = TRUE)

## NLP TECHNIQUES

liwc_alt <- c(
  "liwc",
  "linguistic inquiry and word count",
  "linguistic inquiry & word count",
  "linguistic inquiry word count",
  "linguistic word count"
)

chatgpt_alt <- c(
  "chatgpt", "chat gpts",
  "chat gpt", "chat gpts",
  "openai chatgpt", "openai chatgpts",
  "chat generative pre-trained transformer", "chat generative pre-trained transformers",
  "chat generative pretrained transformer", "chat generative pretrained transformers"
)
bert_alt <- c(
  "bert",
  "bi-directional encoder representation from transformer",
  "bi-directional encoder representation from transformers",
  "bi-directional encoder representations from transformer",
  "bi-directional encoder representations from transformers",
  "bidirectional encoder representation from transformer",
  "bidirectional encoder representation from transformers",
  "bidirectional encoder representations from transformer",
  "bidirectional encoder representations from transformers",
  "bi directional encoder representation from transformer",
  "bi directional encoder representation from transformers",
  "bi directional encoder representations from transformer",
  "bi directional encoder representations from transformers"
)

llm_alt <- c(
  "large language model", "large language models",
  "llm", "llms",
  "large scale language model", "large scale language models"
)

lda_alt <- c(
  "latent dirichlet allocation", "latent dirichlet allocations",
  "lda", "ldas",
  "dirichlet allocation", "dirichlet allocations"
)

stm_alt <- c(
  "structural topic model", "structural topic models",
  "structural topic modeling", "structural topic modelings",
  "structural topic modelling", "structural topic modellings"
)

ner_alt <- c(
  "named entity recognition", "named entity recognitions",
  "entity recognition", "entity recognitions"
)

sentiment_analysis_alt <- c(
  "sentiment analysis"
)

topic_modeling_alt <- c(
  "topic modeling", "topic modelings",
  "topic model", "topic models",
  "topic modelling", "topic modellings"
)

tfidf_alt <- c(
  "tf-idf", "tf-idfs",
  "tfidf", "tfidfs",
  "term frequency-inverse document frequency", "term frequency-inverse document frequencies",
  "term frequency inverse document frequency", "term frequency inverse document frequencies"
)

embeddings_alt <- c(
  "word embedding", "word embeddings",
  "embeddings"
)

transformers_alt <- c(
  "transformer", "transformers",
  "transformer model", "transformer models"
)

roberta_alt <- c(
  "roberta", "robertas",
  "robustly optimized bert approach", "robustly optimized bert approaches"
)

word2vec_alt <- c(
  "word2vec", "word2vecs",
  "word to vec", "word to vecs",
  "word 2 vec", "word 2 vecs",
  "word2vec model", "word2vec models",
  "word to vectors", "word to vector",
  "word 2 vectors", "word 2 vector"
)

fasttext_alt <- c(
  "fasttext", "fasttexts"
)

textrank_alt <- c(
  "textrank", "textranks"
)

gpt2_alt <- c(
  "gpt2", "gpt-2", "gpt 2",
  "generative pre-trained transformer 2", "generative pre-trained transformers 2",
  "generative pretrained transformer 2", "generative pretrained transformers 2"
)

gpt3_alt <- c(
  "gpt3", "gpt-3", "gpt 3",
  "generative pre-trained transformer 3", "generative pre-trained transformers 3",
  "generative pretrained transformer 3", "generative pretrained transformers 3"
)

pos_tagging_alt <- c(
  "pos tagging", "pos taggings",
  "part of speech tagging", "parts of speech tagging",
  "pos tagger", "pos taggers"
)

para_alt <- c(
  "paralanguage classifier", "paralanguage classifiers",
  "textual paralanguage classifier", "textual paralanguage classifiers"
)

leximancer_alt <- c(
  "leximancer"
)

passivepy_alt <- c(
  "passivepy"
)

# count the presence of each technique in the abstracts, title, and keywords
check_presence <- function(text, keywords) {
  sapply(keywords, function(keyword) grepl(keyword, text, ignore.case = TRUE)) %>% any()
}

# Add columns for each technique based on above dictionaries
techniques <- list(LIWC = liwc_alt, ChatGPT = chatgpt_alt, LLM = llm_alt, LDA = lda_alt, STM = stm_alt, 
                   BERT = bert_alt, NER = ner_alt, Sentiment_Analysis = sentiment_analysis_alt, 
                   TFIDF = tfidf_alt, Embeddings = embeddings_alt, Transformers = transformers_alt, 
                   RoBERTa = roberta_alt, Word2Vec = word2vec_alt, FastText = fasttext_alt, 
                   TextRank = textrank_alt, GPT2 = gpt2_alt, GPT3 = gpt3_alt, POS_Tagging = pos_tagging_alt,
                   PARA = para_alt, Leximancer = leximancer_alt, PassivePy = passivepy_alt)

for (technique_name in names(techniques)) {
  list_articles_techniques[[technique_name]] <- sapply(list_articles_techniques$combined_text, check_presence, techniques[[technique_name]])
  list_articles_techniques[[technique_name]] <- as.integer(list_articles_techniques[[technique_name]])
}

# number of techniques per year
techniques_per_year <- list_articles_techniques %>%
  group_by(year) %>%
  summarise(across(names(techniques), sum, na.rm = TRUE)) %>%
  ungroup()

# cumulative number of techniques per year
cumulative_techniques_year <- techniques_per_year %>%
  arrange(year) %>%
  mutate(across(-year, cumsum))

techniques_2023 <- cumulative_techniques_year %>%
  filter(year == 2023) %>% 
  pivot_longer(-year, names_to = "technique", values_to = "count") %>%
  select(-year) %>%
  arrange(desc(count))



top_1O_techniques <- techniques_2023$technique  %>%
  head(10)

top_1O_techniques
top_1O_techniques

palette_colors <- wesanderson::wes_palette("GrandBudapest1", n = 10, type = "continuous")

# Créer le graphe
fig <- ggplot(cumulative_techniques_year, aes(x = year)) +
  geom_line(aes(y = Sentiment_Analysis, color = "Sentiment Analysis")) +
  geom_line(aes(y = LDA, color = "LDA")) +
  geom_line(aes(y = LIWC, color = "LIWC")) +
  geom_line(aes(y = ChatGPT, color = "ChatGPT")) +
  geom_line(aes(y = Embeddings, color = "Embeddings")) +
  geom_line(aes(y = Transformers, color = "Transformers")) +
  geom_line(aes(y = Leximancer, color = "Leximancer")) +
  geom_line(aes(y = BERT, color = "BERT")) +
  geom_line(aes(y = STM, color = "STM")) +
  geom_line(aes(y = Word2Vec, color = "Word2Vec")) +
  scale_color_manual(values = palette_colors) +
  labs(
    title = "Evolution of NLP Techniques in Marketing (Top 10 by 2023)",
    subtitle = "Cumulative sum of the number of articles mentioning each technique",
    x = "",
    y = "Cumulative Number of Occurrences"
  ) +
  xlim(2009, 2023) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())
# pass the ggplot object to plotly
ggplotly(fig)






# Sum the number of techniques used in each article
list_articles_techniques$sum_techniques <- rowSums(list_articles_techniques[names(techniques)])
most_technical_articles <- list_articles_techniques[order(list_articles_techniques$sum_techniques, decreasing = TRUE),] %>%
  select(entry_number, dc_title, 23, year, sum_techniques, Rank, SJR, citedby_count)

```
```{r}
#other technique to plot with label on curves 

Royal2 <- wesanderson::wes_palette("GrandBudapest1", n = 10, type = "continuous")

# Créer le graphe ggplot
fig2 <- ggplot(cumulative_techniques_year, aes(x = year)) +
  geom_line(aes(y = LIWC, color = "LIWC")) +
  geom_line(aes(y = ChatGPT, color = "ChatGPT")) +
  geom_line(aes(y = LLM, color = "LLM")) +
  geom_line(aes(y = LDA, color = "LDA")) +
  geom_line(aes(y = STM, color = "STM")) +
  geom_line(aes(y = BERT, color = "BERT")) +
  geom_line(aes(y = Sentiment_Analysis, color = "Sentiment Analysis")) +
  geom_line(aes(y = Embeddings, color = "Embeddings")) +
  geom_line(aes(y = Transformers, color = "Transformers")) +
  geom_line(aes(y = RoBERTa, color = "RoBERTa")) +
  scale_color_manual(values = Royal2) +
  xlim(2009, 2024) +
  labs(
    title = "Evolution of NLP Techniques (Top 10 by 2023)",
    subtitle = "Cumulative sum of articles mentioning each technique",
    x = "Year",
    y = "Cumulative Count"
  ) +
  theme_minimal() +
  theme(legend.position = "none") # Cacher la légende car les labels sont directement sur les courbes

# Ajouter les labels à la fin de chaque courbe avec geom_text_repel
labels_positions <- data.frame(
  year = 2023, # Année de référence pour positionner les labels
  LIWC = cumulative_techniques_year$LIWC[which.max(cumulative_techniques_year$year)],
  ChatGPT = cumulative_techniques_year$ChatGPT[which.max(cumulative_techniques_year$year)],
  LLM = cumulative_techniques_year$LLM[which.max(cumulative_techniques_year$year)],
  LDA = cumulative_techniques_year$LDA[which.max(cumulative_techniques_year$year)],
  STM = cumulative_techniques_year$STM[which.max(cumulative_techniques_year$year)],
  BERT = cumulative_techniques_year$BERT[which.max(cumulative_techniques_year$year)],
  "Sentiment Analysis" = cumulative_techniques_year$Sentiment_Analysis[which.max(cumulative_techniques_year$year)],
  Embeddings = cumulative_techniques_year$Embeddings[which.max(cumulative_techniques_year$year)],
  Transformers = cumulative_techniques_year$Transformers[which.max(cumulative_techniques_year$year)],
  RoBERTa = cumulative_techniques_year$RoBERTa[which.max(cumulative_techniques_year$year)]
)

# Pour chaque technique, ajouter un label en utilisant geom_text_repel
for(technique in names(labels_positions)[-1]) {
  fig2 <- fig2 + geom_text_repel(data = labels_positions, 
                               aes_string(x = "year", y = technique, label = sprintf("'%s'", technique)),
                               nudge_x = 0.5, nudge_y = 0, force = 10)
}

# Afficher le graphe
print(fig2)
```

## Diffusion and delay of NLP techniques

## To be continued...

-   Qualitative analysis of the authors' communities over time
-   Writing, writing, writing the paper

::: {style="text-align: center"}
### Thank you for your attention
:::

| **Code**                                       | **Slides**                              | **Personal Github**                     |
|---------------------------|-----------------------|-----------------------|
| ![](images/networks_code_qr.png){width="100%"} | ![](images/slides_qr.png){width="100%"} | ![](images/github_qr.png){width="100%"} |

## References

::: {#refs}
:::
